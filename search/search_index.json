{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Hello there! I'm Shreekantha (Shree). I'm currently a Speech Recognition Engineer at Dialpad , building the next-gen ASR product. Before this, I was pursuing MS by Research in Data Science at IIIT-Bangalore under the supervision of Prof. Sachit Rao and Prof. V. Ramasubramanian , where I also worked at the E-Health Research Center (EHRC) and the Machine Intelligence and Robotics Center (MINRO) as a Research Scholar. I graduated with a thesis on \"Multi-task learning in end-to-end attention-based automatic speech recognition\" . Before joining IIIT-Bangalore, I was with Sonus Networks (Now Ribbon Communications ) where we developed Element Management Systems for 4G-VOIP products. I have a B.E. in Telecommunication Engineering from JNNCE Shivamogga (VTU) . My research interests include: Streaming end-to-end ASR for conversational, telephony, and videoconferencing speech Low-latency and computationally constrained scenarios Multi-lingual and code-switched speech recognition Bringing external knowledge into the purely data-driven end-to-end architectures When I'm not building next-gen ASR products, I conduct research to bring in some classical speech knowledge to the purely data-driven models of recent years. Through this exercise, I'm hoping to blend the pure data-driven architectures with speech knowledge, leading to a reduction in model complexity, faster training/inference, and hopefully, deeper insights into speech recognition. Other problems I am currently working on include - KWS using neural attention - A pre-training method for end-to-end ASR - Better training strategies for encoder-attention-decoder models - Interpretability and explainability of end-to-end ASR models - Multilingual and code-switching scenarios - Gathering data and building ASR models for low-resource Indian Languages - Kannada - Sanskrit Most of my work is in kaldi/K2, ESPnet, NeMo, and some custom code in PyTorch \u2764\ufe0f and TensorFlow 2^. When I'm not working on Speech and Language Technology: I love to go cycling Build stuff that interests me Contemplate upon the nature of existence and consciousness. Talk to me about it from all perspectives - materialistic, advaitic, dvaitic, etc. \"\u0905\u0925\u093e\u0924\u094b \u092c\u094d\u0930\u0939\u094d\u092e \u091c\u093f\u091c\u094d\u091e\u093e\u0938\u093e\" (Ath\u0101to brahma jij\u00f1\u0101s\u0101 - Now is the time to inquire about the Absolute Truth). This is me in 2022: You can reach me at shreekantha.nadig@iiitb.ac.in","title":"Home"},{"location":"#home","text":"Hello there! I'm Shreekantha (Shree). I'm currently a Speech Recognition Engineer at Dialpad , building the next-gen ASR product. Before this, I was pursuing MS by Research in Data Science at IIIT-Bangalore under the supervision of Prof. Sachit Rao and Prof. V. Ramasubramanian , where I also worked at the E-Health Research Center (EHRC) and the Machine Intelligence and Robotics Center (MINRO) as a Research Scholar. I graduated with a thesis on \"Multi-task learning in end-to-end attention-based automatic speech recognition\" . Before joining IIIT-Bangalore, I was with Sonus Networks (Now Ribbon Communications ) where we developed Element Management Systems for 4G-VOIP products. I have a B.E. in Telecommunication Engineering from JNNCE Shivamogga (VTU) . My research interests include: Streaming end-to-end ASR for conversational, telephony, and videoconferencing speech Low-latency and computationally constrained scenarios Multi-lingual and code-switched speech recognition Bringing external knowledge into the purely data-driven end-to-end architectures When I'm not building next-gen ASR products, I conduct research to bring in some classical speech knowledge to the purely data-driven models of recent years. Through this exercise, I'm hoping to blend the pure data-driven architectures with speech knowledge, leading to a reduction in model complexity, faster training/inference, and hopefully, deeper insights into speech recognition. Other problems I am currently working on include - KWS using neural attention - A pre-training method for end-to-end ASR - Better training strategies for encoder-attention-decoder models - Interpretability and explainability of end-to-end ASR models - Multilingual and code-switching scenarios - Gathering data and building ASR models for low-resource Indian Languages - Kannada - Sanskrit Most of my work is in kaldi/K2, ESPnet, NeMo, and some custom code in PyTorch \u2764\ufe0f and TensorFlow 2^. When I'm not working on Speech and Language Technology: I love to go cycling Build stuff that interests me Contemplate upon the nature of existence and consciousness. Talk to me about it from all perspectives - materialistic, advaitic, dvaitic, etc. \"\u0905\u0925\u093e\u0924\u094b \u092c\u094d\u0930\u0939\u094d\u092e \u091c\u093f\u091c\u094d\u091e\u093e\u0938\u093e\" (Ath\u0101to brahma jij\u00f1\u0101s\u0101 - Now is the time to inquire about the Absolute Truth). This is me in 2022: You can reach me at shreekantha.nadig@iiitb.ac.in","title":"Home"},{"location":"publications/","text":"\"Avengers, Ensemble! Benefits of ensembling in grapheme-to-phoneme prediction\" (2021). Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology . Vasundhara Gautam, Wang Yau Li, Zafarullah Mahmood, Fred Mailhot, Shreekantha Nadig , Riqiang Wang, and Nathan Zhang. \u201dMulti-target hybrid CTC-Attentional Decoder for joint phoneme-grapheme recognition\u201d (2020). International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2020 . Shreekantha Nadig , V. Ramasubramanian, Sachit Rao. \u201dJointly learning to align and transcribe using attention-based alignment and uncertainty-to-weigh losses\u201d (2020). International Conferenceon Signal Processing and Communications (SPCOM), Bangalore, India, 2020 Shreekantha Nadig , Sumit Chakraborty, Anuj Shah, Chaitanay Sharma, V. Ramasubramanian, Sachit Rao ( Best Student Paper Award -- Honorable Mention ) \"Semi-supervised learning for acoustic model retraining: Handling speech data with noisy transcript\" (2020). International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2020 Abhijith Madan, Ayush Khopkar, Shreekantha Nadig , K. M. Srinivasa Raghavan, V. Ramasubramanian #noneall a{ text-decoration:none !important; }","title":"Publications"},{"location":"recognition/","text":"MUCS 2021: MUltilingual and Code-Switching ASR Challenges for Low Resource Indian Languages Bengaluru, IN | Aug 2021 Third Prize Team presentation Team contributions to multilingual and low-resource ASR for Indian Languages. Benchmarking and open-sourcing various end-to-end methods and studying effects of channel distortions on language identification. Code available here . SPCOM 2020 Bengaluru, IN | Aug 2021 Best Student Paper Award -- Honorable Mention Jointly learning to align and transcribe using attention-based alignment and uncertainty-to-weigh losses JUincubator Hackathon powered by GMASA Bengaluru, IN | Jul 2017 Third Prize Developed a web app iCarto - a serious game for urban planning IEEE PAPYRUS - SJCE Mysore Mysuru, IN | Mar 2014 First Prize Paper presentation - \"Computer Vision Based Tomato Classifier\". PAPYRUS 2014: IEEE-SJCE","title":"Recognition"},{"location":"work/","text":"Dialpad, Inc. Speech Recognition Engineer Dec 2019 - Present Architect the Dialpad next-gen ASR system with streaming end-to-end architectures Lead the R&D on streaming end-to-end ASR for conversational, telephony, and videoconferencing speech under low latency and multi accent scenarios Benchmarked various toolkits including Kaldi, K2, ESPnet, NeMo, and WeNet to architect the next-gen ASR system Built and benchmarked various end-to-end ASR architectures with CTC, Attention-based Encoder-Decoder (AED), Transducer, Transformer, and Conformer models with hybrid ASR models and external ASR services Developed interfaces for the shallow fusion of multi-level (sub-word and word) RNNLMs and n-gram LMs Developed methods to bias the models towards a list of keywords, resulting in an absolute WERR of 7% Automated the data preparation pipeline for training ASR models, reducing the turnaround time for experiments and increasing productivity of the team Developed pronunciation-assisted sub-word models using fast-align, GIZA++, and Pynini, resulting in an absolute WERR of 3% compared to BPE sub-words Post-training quantization of ASR models to achieve 50% faster RTF and 75% smaller models on disk Developed performance monitoring techniques for end-to-end ASR models based on RNN-AED and CTC confidence scores, and their efficacy in semi-supervised and self-supervised learning techniques Developed better endpoint detection for hybrid models and achieved 4% relative WERR Developed a web-app for internal users to query production calls and visualize hypotheses using wavesurfer-js Open-source contributions: MUCS 2021 : MUltilingual and Code-Switching ASR Challenges for Low Resource Indian Languages Third Prize in the challenge Team contributions to multilingual and low-resource ASR for Indian Languages. Benchmarking and open-sourcing various end-to-end methods and studying effects of channel distortions on language identification Code available here Observe AI Machine Learning Intern - ASR May 2019 - Aug 2019 Developed a feature extraction pipeline using tf.signal and tf.data Implemented different keyword-spotting (KWS) papers - Deep-KWS, CTC KWS Developed methods to convert a custom PyTorch model to TensorFlow Deployed the KWS model using TensorFlow serving with an RTF of 0.05 on GPU IIIT-Bangalore Research Scholar Jan 2017 - Dec 2019 Developed end-to-end methods for multilingual and code-switching scenarios in Indian Languages Developed joint ASR and KWS systems using joint phoneme-grapheme recognition Developed a more accurate and faster training method by jointly training alignment and ASR model Mentored MTech and iMTech students in their projects and thesis work and delivered various tutorials and talks around ASR Developed a remote hardware laboratory to study control systems using embedded programming and web technologies Developed HCI visualizations for a humanoid using Unity and C# I was also involved in different labs and activities including: - E-Health Research Center (EHRC) - Developed rehabilitation robotics applications - Machine Intelligence and Robotics Center (MINRO) - Developed multi-lingual applications of Speech and Language Technologies in the domain of e-governance. - Intel AI Academy Student Ambassador . - Built a small-footprint ASR application ( keyword spotting , wake-word detection) on the \"edge\" using Intel's Neural Compute Stick 2 (NCS2) and OpenVINO. - Graduate Teaching Assistant - Deep Learning for Automatic Speech Recognition - Automatic Speech Recognition - Introduction to Robotics Invited Talks IIIT-B Samvaad talk Bengaluru, IN | Dec 2020 Multi-task learning in end-to-end attention-based automatic speech recognition (MS Thesis) Open challenges in multi-task learning for ASR IIIT-B Guest Lecture Series - Deep Learning for ASR Bengaluru, IN | Sep 2020 - Dec 2020 Discussions on RNN-CTC, RNN-AED, RNN-T, and Transformer models for ASR Discussions on model quantization and weight sparsity in RNN-T models for low computational resource and latency constraints Unpacking and analyzing the Pixel Recorder app to showcase how tflite models are packed with custom TensorFlow ops Artificial Intelligence : A Way Forward Bengaluru, IN | Sep 2019 Faculty development program at Dayananda Sagar College of Arts, Science and Commerce, Bangalore Discussions on the use of AI in speech and language technology TCS Think Labs Bengaluru, IN | Feb 2019 Motivation and introduction to end-to-end ASR Discussion on the topics of RNN, CTC, Attention and LM fusion IIIT-B AI Reading Group Bengaluru, IN | Nov 2018 Discussions on various attention models in end-to-end ASR Semi-supervised learning with end-to-end ASR models BMSCE AI Workshop Bengaluru, IN | Sep 2018 Artificial Intelligence and Deep Neural Networks Workshop for undergraduate students at BMS College of Engineering, Bangalore Code examples and tutorials in TensorFlow Keras Sonus Networks SVT Engineer Aug 2015 - Jan 2017 Worked as a part of Sustaining SVT on Real-Time communication products Sonus Insight (EMS) and SBC Developed automated test frameworks in Python, Perl, Linux, and Java Worked with CentOS, Red Hat Enterprise Linux, and Solaris to develop and test the products Developed tools that reduced team effort from many hours to a couple of minutes .alignleft { float: left; } .alignright { float: right; }","title":"Work"},{"location":"work/#dialpad-inc","text":"Speech Recognition Engineer Dec 2019 - Present Architect the Dialpad next-gen ASR system with streaming end-to-end architectures Lead the R&D on streaming end-to-end ASR for conversational, telephony, and videoconferencing speech under low latency and multi accent scenarios Benchmarked various toolkits including Kaldi, K2, ESPnet, NeMo, and WeNet to architect the next-gen ASR system Built and benchmarked various end-to-end ASR architectures with CTC, Attention-based Encoder-Decoder (AED), Transducer, Transformer, and Conformer models with hybrid ASR models and external ASR services Developed interfaces for the shallow fusion of multi-level (sub-word and word) RNNLMs and n-gram LMs Developed methods to bias the models towards a list of keywords, resulting in an absolute WERR of 7% Automated the data preparation pipeline for training ASR models, reducing the turnaround time for experiments and increasing productivity of the team Developed pronunciation-assisted sub-word models using fast-align, GIZA++, and Pynini, resulting in an absolute WERR of 3% compared to BPE sub-words Post-training quantization of ASR models to achieve 50% faster RTF and 75% smaller models on disk Developed performance monitoring techniques for end-to-end ASR models based on RNN-AED and CTC confidence scores, and their efficacy in semi-supervised and self-supervised learning techniques Developed better endpoint detection for hybrid models and achieved 4% relative WERR Developed a web-app for internal users to query production calls and visualize hypotheses using wavesurfer-js","title":"Dialpad, Inc."},{"location":"work/#open-source-contributions","text":"MUCS 2021 : MUltilingual and Code-Switching ASR Challenges for Low Resource Indian Languages Third Prize in the challenge Team contributions to multilingual and low-resource ASR for Indian Languages. Benchmarking and open-sourcing various end-to-end methods and studying effects of channel distortions on language identification Code available here","title":"Open-source contributions:"},{"location":"work/#observe-ai","text":"Machine Learning Intern - ASR May 2019 - Aug 2019 Developed a feature extraction pipeline using tf.signal and tf.data Implemented different keyword-spotting (KWS) papers - Deep-KWS, CTC KWS Developed methods to convert a custom PyTorch model to TensorFlow Deployed the KWS model using TensorFlow serving with an RTF of 0.05 on GPU","title":"Observe AI"},{"location":"work/#iiit-bangalore","text":"Research Scholar Jan 2017 - Dec 2019 Developed end-to-end methods for multilingual and code-switching scenarios in Indian Languages Developed joint ASR and KWS systems using joint phoneme-grapheme recognition Developed a more accurate and faster training method by jointly training alignment and ASR model Mentored MTech and iMTech students in their projects and thesis work and delivered various tutorials and talks around ASR Developed a remote hardware laboratory to study control systems using embedded programming and web technologies Developed HCI visualizations for a humanoid using Unity and C# I was also involved in different labs and activities including: - E-Health Research Center (EHRC) - Developed rehabilitation robotics applications - Machine Intelligence and Robotics Center (MINRO) - Developed multi-lingual applications of Speech and Language Technologies in the domain of e-governance. - Intel AI Academy Student Ambassador . - Built a small-footprint ASR application ( keyword spotting , wake-word detection) on the \"edge\" using Intel's Neural Compute Stick 2 (NCS2) and OpenVINO. - Graduate Teaching Assistant - Deep Learning for Automatic Speech Recognition - Automatic Speech Recognition - Introduction to Robotics","title":"IIIT-Bangalore"},{"location":"work/#invited-talks","text":"IIIT-B Samvaad talk Bengaluru, IN | Dec 2020 Multi-task learning in end-to-end attention-based automatic speech recognition (MS Thesis) Open challenges in multi-task learning for ASR IIIT-B Guest Lecture Series - Deep Learning for ASR Bengaluru, IN | Sep 2020 - Dec 2020 Discussions on RNN-CTC, RNN-AED, RNN-T, and Transformer models for ASR Discussions on model quantization and weight sparsity in RNN-T models for low computational resource and latency constraints Unpacking and analyzing the Pixel Recorder app to showcase how tflite models are packed with custom TensorFlow ops Artificial Intelligence : A Way Forward Bengaluru, IN | Sep 2019 Faculty development program at Dayananda Sagar College of Arts, Science and Commerce, Bangalore Discussions on the use of AI in speech and language technology TCS Think Labs Bengaluru, IN | Feb 2019 Motivation and introduction to end-to-end ASR Discussion on the topics of RNN, CTC, Attention and LM fusion IIIT-B AI Reading Group Bengaluru, IN | Nov 2018 Discussions on various attention models in end-to-end ASR Semi-supervised learning with end-to-end ASR models BMSCE AI Workshop Bengaluru, IN | Sep 2018 Artificial Intelligence and Deep Neural Networks Workshop for undergraduate students at BMS College of Engineering, Bangalore Code examples and tutorials in TensorFlow Keras","title":"Invited Talks"},{"location":"work/#sonus-networks","text":"SVT Engineer Aug 2015 - Jan 2017 Worked as a part of Sustaining SVT on Real-Time communication products Sonus Insight (EMS) and SBC Developed automated test frameworks in Python, Perl, Linux, and Java Worked with CentOS, Red Hat Enterprise Linux, and Solaris to develop and test the products Developed tools that reduced team effort from many hours to a couple of minutes .alignleft { float: left; } .alignright { float: right; }","title":"Sonus Networks"},{"location":"blog/","text":"Tensorflow 2.0 tf.data.Dataset.from_generator 2019 My findings about the new TensorFlow 2.0 Dataset API Attention models in ESPnet toolkit for Speech Recognition 2019 Detailed discussion of Attention models for Speech Recognition in ESPnet toolkit. Introduction to attention models for speech recognition 2019 Introduction to Attention models and the differences with the Encoder-Attention-Decoder framework Encoder-Decoder framework for Speech Recognition 2019 Introduction to Encoder-Decoder framework and it's significance","title":"Blog"},{"location":"blog/#tensorflow-20-tfdatadatasetfrom_generator","text":"2019 My findings about the new TensorFlow 2.0 Dataset API","title":"Tensorflow 2.0 tf.data.Dataset.from_generator"},{"location":"blog/#attention-models-in-espnet-toolkit-for-speech-recognition","text":"2019 Detailed discussion of Attention models for Speech Recognition in ESPnet toolkit.","title":"Attention models in ESPnet toolkit for Speech Recognition"},{"location":"blog/#introduction-to-attention-models-for-speech-recognition","text":"2019 Introduction to Attention models and the differences with the Encoder-Attention-Decoder framework","title":"Introduction to attention models for speech recognition"},{"location":"blog/#encoder-decoder-framework-for-speech-recognition","text":"2019 Introduction to Encoder-Decoder framework and it's significance","title":"Encoder-Decoder framework for Speech Recognition"},{"location":"blog/2019/attention/","text":"Attention models in ESPnet toolkit for Speech Recognition TL;DR - Different attention mechanisms available in the ESPnet toolkit explained. Have a look at the presentation that I gave in IIIT-B AI reading group (no math included) Attention based models in End-to-End ASR I'll directly jump to explaining the different Attention models available in the ESPnet toolkit. (I won't be going into the implementation challenges in getting the Encoder-Decoer Attention models work.) Please have a look at the previous post for the basics of Attention models in Speech recognition. This post assumes you know the Attention mechanism in general and build from there. No Attention Content-based Attention Dot product Attention Additive Attention Location-aware Attention Location Aware Attention 2D Location Aware Attention Location Aware Recurrent Attention Hybrid Attention Coverage Mechanism Attention Coverage Mechanism Location Aware Attention Multi-Head Attention Multi-Head dot product Attention Multi-Head additive Attention Multi-Head Location Aware Attention Multi-Head Multi-Resolution Location Aware Attention Attention - Recap \\(x = (x_{1}, x_{2}, .........., x_{T})\\) - is the input sequence \\(y = (y_{1}, y_{2}, .........., y_{U})\\) - is the target output sequence \\(h = (h_{1}, h_{2}, .........., h_{T})\\) - is the output of the Encoder \\(h_{t} = f(x_{t}, h_{t-1})\\) - is the Encoder function \\(C_{i} = \\sum_{j=1}^{T} \\alpha_{i,j} \\cdot h_{j}\\) - is the Context vector \\(\\alpha_{i,j} = Softmax(e_{i,j}) = \\frac{e^{e_{i,j}}}{\\sum_{k=1}^{T} e^{e_{i,k}}}\\) - are the Attention weights \\(e_{i,j} = a(s_{i-1}, h_j)\\) - is the importance parameter for every encoded input \\(\\sum_{j=1}^{T} e_{i,j} \\neq 1\\) - the importance parameter need not sum to 1 \\(\\sum_{j=1}^{T} \\alpha_{i,j} = 1\\) - the attention weights sum to 1 Types of Attention Broadly, attention mechanisms can be categorized into 3 distinct categories Content aware Attention Location aware Attention Hybrid Attention Multi-Head Attention mechanisms are a different beast altogether, we will cross that bridge when we get there. For now, let's concentrate on the 3 broad categories I mentioned. No Attention (Equal Attention?) Here, no attention is used at all. Each of the \\(h_{i}\\) are given equal importance and linearly mixed and averaged to get \\(C_{i}\\) \\[e_{t} = \\frac{1}{T}\\] \\[C_{i} = \\sum_{j=1}^{T} \\frac{1}{T} h_{j}\\] No attention - code Python 1 2 3 4 5 #Mask = Ones where enc_h is present. Zeros where padding is needed. mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev = mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ) att_prev = att_prev . to ( enc_h ) c = torch . sum ( enc_h * att_prev . view ( batch , h_length , 1 ), dim = 1 ) No attention - full picture Content-based Attention Content-based Attention - as the name suggests is based on the contents of the vector \\(s_{i-1}\\) (Decoder hidden state) and \\(h_{t}\\) (Annotation vectors from the Encoder). This means, our compatibility function or the Attention function depends only on the contents of these vectors, irrespective of their location in the sequence. What does this mean? Let's say what has been spoken in the utterance is Barb burned paper and leaves in a big bonfire. with the phonetic sequence as sil b aa r sil b er n sil p ey sil p er n l iy v z ih n ah sil b ih sil b aa n f ay er sil . The feature vector of a phoneme, let's say b will be similar no matter the location of the phoneme in the sequence sil b aa r sil b er n sil p ey sil p er n l iy v z ih n ah sil b ih sil b aa n f ay er sil This would give equal weight to the same phoneme, but from a different word which is not relevant to the current context. Also, a phonetically similar phoneme will get a close score to the actual phoneme. Content-based Attention is computed as: \\[ \\begin{equation} e_{i,j} = a(h_{j}, s_{i-1}) \\end{equation} \\] Dot product and additive attention are content-based attention mechanisms. 2. Dot product Attention In the dot product attention, our similarity measure is the dot product between the vector \\(s_{i-1}\\) and \\(h_{t}\\) . For generating the Context vector \\(C_{i}\\) , we take the Decoder hidden state \\(s_{i-1}\\) when generating the previous output symbol \\(y_{i-1}\\) and compute the dot product with each \\(h_{t}\\) to get \\(e_{i,j}\\) for each of the Annotation vectors. Conceptually dot product signifies how similar each vectors are (the angle between them). More similar they are, higher the value. Here's an image explaining Dot Product Attention Here, dec_z vector is the Decoder hidden state. As we discussed in the previous post , these representations are in different dimensions. So, we learn a transformation to transform them to same dimensions so that we can compare them using dot product or addition. This transformation is learnt with other parameters using backprop. Dot product attention - code Python 1 2 3 4 5 6 7 8 9 mlp_enc = torch . nn . Linear ( eprojs , att_dim ) mlp_dec = torch . nn . Linear ( dunits , att_dim ) pre_compute_enc_h = torch . tanh ( mlp_enc ( enc_h )) e = torch . sum ( pre_compute_enc_h * torch . tanh ( mlp_dec ( dec_z )) . view ( batch , 1 , att_dim ), dim = 2 ) w = F . softmax ( scaling * e , dim = 1 ) c = torch . sum ( enc_h * w . view ( batch , h_length , 1 ), dim = 1 ) Dot product attention - full picture If we are computing the attention weights based on only the contents of the vectors from Decoder and Encoder, similar Annotation vectors get weighed equally irrespective of the position. We can see this clearly from the Attention plots from the model. Observe in the following image how the Attention weights are not monotonic and tend to be distributed near positions where the Annotation vectors are similar in the acoustic space. We could also plot where the model is attending to for generating each output symbol. Here, I have added an overlay for each row of the first image just to highlight which output symbol is being generated. The actual attention weights look like the above image. We could also correlate this with the spectrogram of the utterance, since we know how much sub-sampling was done in the model. I have used a sub-sampling of 1_2_2_1_1 . In our utterance FJSJ0_SX404, if we use a window size of 250ms and a frame shift of 10ms, we get 240 frames of feature vectors. Because of sub-sampling in our model, these features are mapped to 60 feature vectors after the Encoder network.","title":"Attention models in ESPnet toolkit for Speech Recognition"},{"location":"blog/2019/attention/#attention-models-in-espnet-toolkit-for-speech-recognition","text":"TL;DR - Different attention mechanisms available in the ESPnet toolkit explained. Have a look at the presentation that I gave in IIIT-B AI reading group (no math included) Attention based models in End-to-End ASR I'll directly jump to explaining the different Attention models available in the ESPnet toolkit. (I won't be going into the implementation challenges in getting the Encoder-Decoer Attention models work.) Please have a look at the previous post for the basics of Attention models in Speech recognition. This post assumes you know the Attention mechanism in general and build from there. No Attention Content-based Attention Dot product Attention Additive Attention Location-aware Attention Location Aware Attention 2D Location Aware Attention Location Aware Recurrent Attention Hybrid Attention Coverage Mechanism Attention Coverage Mechanism Location Aware Attention Multi-Head Attention Multi-Head dot product Attention Multi-Head additive Attention Multi-Head Location Aware Attention Multi-Head Multi-Resolution Location Aware Attention","title":"Attention models in ESPnet toolkit for Speech Recognition"},{"location":"blog/2019/attention/#attention-recap","text":"\\(x = (x_{1}, x_{2}, .........., x_{T})\\) - is the input sequence \\(y = (y_{1}, y_{2}, .........., y_{U})\\) - is the target output sequence \\(h = (h_{1}, h_{2}, .........., h_{T})\\) - is the output of the Encoder \\(h_{t} = f(x_{t}, h_{t-1})\\) - is the Encoder function \\(C_{i} = \\sum_{j=1}^{T} \\alpha_{i,j} \\cdot h_{j}\\) - is the Context vector \\(\\alpha_{i,j} = Softmax(e_{i,j}) = \\frac{e^{e_{i,j}}}{\\sum_{k=1}^{T} e^{e_{i,k}}}\\) - are the Attention weights \\(e_{i,j} = a(s_{i-1}, h_j)\\) - is the importance parameter for every encoded input \\(\\sum_{j=1}^{T} e_{i,j} \\neq 1\\) - the importance parameter need not sum to 1 \\(\\sum_{j=1}^{T} \\alpha_{i,j} = 1\\) - the attention weights sum to 1","title":"Attention - Recap"},{"location":"blog/2019/attention/#types-of-attention","text":"Broadly, attention mechanisms can be categorized into 3 distinct categories Content aware Attention Location aware Attention Hybrid Attention Multi-Head Attention mechanisms are a different beast altogether, we will cross that bridge when we get there. For now, let's concentrate on the 3 broad categories I mentioned.","title":"Types of Attention"},{"location":"blog/2019/attention/#no-attention-equal-attention","text":"Here, no attention is used at all. Each of the \\(h_{i}\\) are given equal importance and linearly mixed and averaged to get \\(C_{i}\\) \\[e_{t} = \\frac{1}{T}\\] \\[C_{i} = \\sum_{j=1}^{T} \\frac{1}{T} h_{j}\\]","title":"No Attention (Equal Attention?)"},{"location":"blog/2019/attention/#no-attention-code","text":"Python 1 2 3 4 5 #Mask = Ones where enc_h is present. Zeros where padding is needed. mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev = mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ) att_prev = att_prev . to ( enc_h ) c = torch . sum ( enc_h * att_prev . view ( batch , h_length , 1 ), dim = 1 )","title":"No attention - code"},{"location":"blog/2019/attention/#no-attention-full-picture","text":"","title":"No attention - full picture"},{"location":"blog/2019/attention/#content-based-attention","text":"Content-based Attention - as the name suggests is based on the contents of the vector \\(s_{i-1}\\) (Decoder hidden state) and \\(h_{t}\\) (Annotation vectors from the Encoder). This means, our compatibility function or the Attention function depends only on the contents of these vectors, irrespective of their location in the sequence. What does this mean? Let's say what has been spoken in the utterance is Barb burned paper and leaves in a big bonfire. with the phonetic sequence as sil b aa r sil b er n sil p ey sil p er n l iy v z ih n ah sil b ih sil b aa n f ay er sil . The feature vector of a phoneme, let's say b will be similar no matter the location of the phoneme in the sequence sil b aa r sil b er n sil p ey sil p er n l iy v z ih n ah sil b ih sil b aa n f ay er sil This would give equal weight to the same phoneme, but from a different word which is not relevant to the current context. Also, a phonetically similar phoneme will get a close score to the actual phoneme. Content-based Attention is computed as: \\[ \\begin{equation} e_{i,j} = a(h_{j}, s_{i-1}) \\end{equation} \\] Dot product and additive attention are content-based attention mechanisms.","title":"Content-based Attention"},{"location":"blog/2019/attention/#2-dot-product-attention","text":"In the dot product attention, our similarity measure is the dot product between the vector \\(s_{i-1}\\) and \\(h_{t}\\) . For generating the Context vector \\(C_{i}\\) , we take the Decoder hidden state \\(s_{i-1}\\) when generating the previous output symbol \\(y_{i-1}\\) and compute the dot product with each \\(h_{t}\\) to get \\(e_{i,j}\\) for each of the Annotation vectors. Conceptually dot product signifies how similar each vectors are (the angle between them). More similar they are, higher the value. Here's an image explaining Dot Product Attention Here, dec_z vector is the Decoder hidden state. As we discussed in the previous post , these representations are in different dimensions. So, we learn a transformation to transform them to same dimensions so that we can compare them using dot product or addition. This transformation is learnt with other parameters using backprop.","title":"2. Dot product Attention"},{"location":"blog/2019/attention/#dot-product-attention-code","text":"Python 1 2 3 4 5 6 7 8 9 mlp_enc = torch . nn . Linear ( eprojs , att_dim ) mlp_dec = torch . nn . Linear ( dunits , att_dim ) pre_compute_enc_h = torch . tanh ( mlp_enc ( enc_h )) e = torch . sum ( pre_compute_enc_h * torch . tanh ( mlp_dec ( dec_z )) . view ( batch , 1 , att_dim ), dim = 2 ) w = F . softmax ( scaling * e , dim = 1 ) c = torch . sum ( enc_h * w . view ( batch , h_length , 1 ), dim = 1 )","title":"Dot product attention - code"},{"location":"blog/2019/attention/#dot-product-attention-full-picture","text":"If we are computing the attention weights based on only the contents of the vectors from Decoder and Encoder, similar Annotation vectors get weighed equally irrespective of the position. We can see this clearly from the Attention plots from the model. Observe in the following image how the Attention weights are not monotonic and tend to be distributed near positions where the Annotation vectors are similar in the acoustic space. We could also plot where the model is attending to for generating each output symbol. Here, I have added an overlay for each row of the first image just to highlight which output symbol is being generated. The actual attention weights look like the above image. We could also correlate this with the spectrogram of the utterance, since we know how much sub-sampling was done in the model. I have used a sub-sampling of 1_2_2_1_1 . In our utterance FJSJ0_SX404, if we use a window size of 250ms and a frame shift of 10ms, we get 240 frames of feature vectors. Because of sub-sampling in our model, these features are mapped to 60 feature vectors after the Encoder network.","title":"Dot product attention - full picture"},{"location":"blog/2019/basics-attention/","text":"Introduction to attention models for speech recognition In the previous post we discussed the Encoder-Decoder framework for Speech Recognition. So, why do we need Attention? What's wrong with the Encoder-Decoder framework? As we discussed in the Encoder-Decoder framework: \\(x = (x_{1}, x_{2}, .........., x_{T})\\) be a length \\(T\\) input feature vector sequence to the Encoder network. \\(y = (y_{1}, y_{2}, .........., y_{U})\\) be a length \\(U\\) output symbol sequence the Decoder (also called the Generator) network generates. \\(h = (h_{1}, h_{2}, .........., h_{T})\\) be the Encoder network output which is the encoded hidden vector sequence of length \\(T\\) . Each encoded representation (annotation) \\(h_{t}\\) contains information about the input sequence with focus on the \\(t^{th}\\) input of the sequence. In the Encoder-Decoder framework, the Encoder tries to summarize the entire input sequence in a fixed dimension vector \\(h_{t}\\) . Potential issues with Encoder-Decoder The neural network needs to be able to compress all the necessary information of the input feature vector sequence into a fixed dimension vector When the sequence is long, especially when the input sequence at test time is significantly longer than the training ones, the performance of the basic Encoder-Decoder network degrades. Also, it is my opinion that forcing the Encoder to summarize the entire feature vector sequence into a fixed dimension vector depends on the size of the vector (longer the sentence - longer the vector) which we can't fix as the sequence length can vary significantly. Attention! One of the solutions to this problem that people have been proposing is the use of Attention. Basically, Attention is an extension to the Encoder-Decoder framework. Each time the model needs to generate an output symbol, it (soft-) searches for a set of positions in the input feature vector sequence where the most relevant information is concentrated. {: .notice--info} We are now concerned with making the model select these set of positions in the input sequence accurately . The main difference with the Encoder-Decoder framework is that here we are not trying to summarize the entire input sequence into a fixed dimension vector. We know from the Encoder-Decoder post that the Encoder is a Recurrent neural network (RNN/LSTM/BLSTM/GRU) and \\(h_{t}\\) is the Encoder hidden state at time \\(t\\) which is computed as: \\[ \\begin{equation} h_{t} = f(x_{t}, h_{t-1}) \\end{equation} \\] Now, instead of feeding the hidden representation \\(h_{T}\\) , let us select a subset of \\(h\\) which are most relevant to a particular context to help the Decoder network generate the output. We linearly blend these relevant \\(h_{t}\\) to get what we refer to as the Context vector \\(C_{i}\\) \\[ \\begin{equation} C_{i} = q(\\{h_{1}, h_{2}, .........., h_{T}\\}, \\alpha_{i}) \\end{equation} \\] Attention: In a way, the model is attending to a subset of the input features which are most relevant to the current context. In all the deep learning techniques, we would like the functions to be differentiable so that we can learn them using backprop. To make this technique of attention to a subset differentiable, we attend to all the input feature vectors, but with different weight! Differences with the Encoder-Decoder network In the Encoder-Decoder network that we discussed in the previous post, the Decoder hidden state is computed as: $$ \\begin{equation} s_{i} = f(s_{i-1}, y_{i-1}) \\end{equation} $$ In the Attention extension, we take the Context vector in computing the Decoder hidden state: $$ \\begin{equation} s_{i} = f(s_{i-1}, y_{i-1}, C_{i}) \\end{equation} $$ The Context vector is the summary of only the most relevant input feature vectors. To capture this relevance , let's consider a variable \\(\\alpha\\) where \\(\\alpha_{i}\\) represents the weight of the encoded representation (also referred to as the annotation ) \\(h_{i}\\) in the Context vector \\(C_{i}\\) - for predicting the output at time \\(i\\) . Given this \\(\\alpha\\) , we can compute the Context vector as: $$ \\begin{equation} C_{i} = \\sum_{j=1}^{T} \\alpha_{i,j} \\cdot h_{j} \\end{equation} $$ \\[ \\begin{equation} \\sum_{j=1}^{T} \\alpha_{i,j} = 1 \\end{equation} \\] To compute \\(\\alpha_{i,j}\\) , we need \\(e_{i,j}\\) - the importance of the \\(j^{th}\\) annotation vector for predicting the \\(i^{th}\\) output symbol. This is what the compatibility function produces. The weight \\(\\alpha_{i,j}\\) of each annotation \\(h_{j}\\) is computed as: $$ \\begin{equation} \\alpha_{i,j} = Softmax(e_{i,j}) = \\frac{e^{e_{i,j}}}{\\sum_{k=1}^{T} e^{e_{i,k}}} \\end{equation} $$ $$ \\begin{equation} \\sum_{j=1}^{T} e_{i,j} \\neq 1 \\end{equation} $$ Where \\(e_{i,j} = a(s_{i-1}, h_j)\\) , \\(a\\) is a compatibility function which computes the importance of each annotation \\(h_j\\) with the Decoder hidden state \\(s_{i-1}\\) . In all our Attention models, it is this function \\(a()\\) that is going to be different. \\(a()\\) defines what type of Attention it is. {: .notice--info} This image summarizes the Attention mechanism. Observe each annotation vector is scaled by the attention weight \\(\\alpha_{i,j}\\) In the Encoder-Decoder network - Given the Decoder hidden representation \\(s_{i-1}\\) (from the previous output time) and the output symbol \\(y_{i-1}\\) (the previous output symbol), we can predict the output symbol at the current time step as: $$ \\begin{equation} p(y_{i} | {y_1, y_2, .........., y_{i-1}}) = g(y_{i-1}, s_i) \\end{equation} $$ Where \\(g()\\) is the entire Decoder function. In the Attention extension - Given the Context vector \\(C_{i}\\) , the Decoder hidden representation \\(s_{i-1}\\) (from the previous output time) and the output symbol \\(y_{i-1}\\) (the previous output symbol), we can predict the output symbol at the current time step as: $$ \\begin{equation} p(y_{i} | {y_1, y_2, .........., y_{i-1}}, C_{i}) = g(y_{i-1}, s_i, C_{i}) \\end{equation} $$ Where \\(g()\\) is the entire Decoder function. The probability of the full output sequence \\(y\\) can be computed as: $$ \\begin{equation} p(y) = \\prod_{i=1}^{U} p(y_i | {y_1, y_2, .........., y_{i-1}}, C_{i}) \\end{equation} $$ Attention weights visualization So, what does Attention even look like? I trained an Attention model on the TIMIT dataset using the ESPnet toolkit and visualized the weights for 20 epochs and this is what it looks like for the speaker FJSJ0 and utterance SX404 of TIMIT: Word transcript for FJSJ0_SX404 : Barb burned paper and leaves in a big bonfire. {: .notice--info} Phoneme transcript for FJSJ0_SX404 : sil b aa r sil b er n sil p ey sil p er n l iy v z ih n ah sil b ih sil b aa n f ay er sil {: .notice--info} Phoneme decoding - final weights Character decoding - final weights On the \\(x\\) axis from left to right is the Encoder index ranging from \\(0\\) to \\(T\\) , where \\(T\\) is the length of the input feature vector sequence. On the \\(y\\) axis from top to bottom is the Decoder index ranging from \\(0\\) to \\(U\\) , where \\(U\\) is the length of the output symbol sequence. Here, you can see that each row corresponds to the weight for each input feature vector \\(h_{t}\\) in producing the Context vector \\(C_{i}\\) for generating the output symbol \\(y_{i}\\) . If you see the Attention weights before the model is trained (at epoch 0), the Attention weights are all random and hence the Context vector \\(C_{i}\\) contains unnecessary noise from irrelevant input feature vectors. This leads to a degraded performance of the model. It is fairly evident that a good Attention model produces a better Context vector which leads to better model performance. Phoneme decoding - initial weights Character decoding - initial weights Attention weights for single output symbol I'm working on visualizing the Attention weights over a Spectrogram every time an output symbol is generated. Ideally it should look like a Gaussian distribution with it's mean at the most relevant \\(h_{t}\\) for generating \\(y_{i}\\) and it's variance proportional to the duration of the phoneme utterance. This is proving more involved than I initially thought, requiring changes to the ESPnet code at a deeper level. I will update this post when I have that. Update: If we plot the Attention weights over the annotation sequence \\(h_{t}\\) for generating each \\(y_{i}\\) , we could see how Attention is playing a role in producing the Context vector \\(C_{i}\\) . Here's what the Attention weights look like for generating each \\(y_{i}\\) at epoch 1. Here's what the Attention weights look like for generating each \\(y_{i}\\) at epoch 20. We could also see how Attention weights progress over time (epochs) to get deeper understanding of how the model is learning. I did just that combining all the Attention weights from each epoch into a gif. Here's what it looks like: Phoneme decoding - Attention weights over epochs Character decoding - Attention weights over epochs Before we start with the different Attention models In all the subsequent discussions of Attention models, I would like to follow some consistency. For example, anything that is orange in color is related to the Encoder side of the network, blue with Decoder side and green with the Attention function itself. We will see often that the representation (annotation) learnt by the Encoder, the hidden state of the Decoder and the representations learnt by the Attention function are of different dimensions. This means we can't add them or take dot product. \\[ \\begin{equation} dim(h_{t}) \\ne dim(s_{i}) \\ne dim(f(e)) \\end{equation} \\] To overcome this issue, we project each of these vectors to a fixed dimension and this non-linear projection is learnt along with the other parameters of the network. In the next post we will discuss about the different Attention models available in the ESPnet toolkit.","title":"Introduction to attention models for speech recognition"},{"location":"blog/2019/basics-attention/#introduction-to-attention-models-for-speech-recognition","text":"In the previous post we discussed the Encoder-Decoder framework for Speech Recognition. So, why do we need Attention? What's wrong with the Encoder-Decoder framework? As we discussed in the Encoder-Decoder framework: \\(x = (x_{1}, x_{2}, .........., x_{T})\\) be a length \\(T\\) input feature vector sequence to the Encoder network. \\(y = (y_{1}, y_{2}, .........., y_{U})\\) be a length \\(U\\) output symbol sequence the Decoder (also called the Generator) network generates. \\(h = (h_{1}, h_{2}, .........., h_{T})\\) be the Encoder network output which is the encoded hidden vector sequence of length \\(T\\) . Each encoded representation (annotation) \\(h_{t}\\) contains information about the input sequence with focus on the \\(t^{th}\\) input of the sequence. In the Encoder-Decoder framework, the Encoder tries to summarize the entire input sequence in a fixed dimension vector \\(h_{t}\\) .","title":"Introduction to attention models for speech recognition"},{"location":"blog/2019/basics-attention/#potential-issues-with-encoder-decoder","text":"The neural network needs to be able to compress all the necessary information of the input feature vector sequence into a fixed dimension vector When the sequence is long, especially when the input sequence at test time is significantly longer than the training ones, the performance of the basic Encoder-Decoder network degrades. Also, it is my opinion that forcing the Encoder to summarize the entire feature vector sequence into a fixed dimension vector depends on the size of the vector (longer the sentence - longer the vector) which we can't fix as the sequence length can vary significantly.","title":"Potential issues with Encoder-Decoder"},{"location":"blog/2019/basics-attention/#attention","text":"One of the solutions to this problem that people have been proposing is the use of Attention. Basically, Attention is an extension to the Encoder-Decoder framework. Each time the model needs to generate an output symbol, it (soft-) searches for a set of positions in the input feature vector sequence where the most relevant information is concentrated. {: .notice--info} We are now concerned with making the model select these set of positions in the input sequence accurately . The main difference with the Encoder-Decoder framework is that here we are not trying to summarize the entire input sequence into a fixed dimension vector. We know from the Encoder-Decoder post that the Encoder is a Recurrent neural network (RNN/LSTM/BLSTM/GRU) and \\(h_{t}\\) is the Encoder hidden state at time \\(t\\) which is computed as: \\[ \\begin{equation} h_{t} = f(x_{t}, h_{t-1}) \\end{equation} \\] Now, instead of feeding the hidden representation \\(h_{T}\\) , let us select a subset of \\(h\\) which are most relevant to a particular context to help the Decoder network generate the output. We linearly blend these relevant \\(h_{t}\\) to get what we refer to as the Context vector \\(C_{i}\\) \\[ \\begin{equation} C_{i} = q(\\{h_{1}, h_{2}, .........., h_{T}\\}, \\alpha_{i}) \\end{equation} \\] Attention: In a way, the model is attending to a subset of the input features which are most relevant to the current context. In all the deep learning techniques, we would like the functions to be differentiable so that we can learn them using backprop. To make this technique of attention to a subset differentiable, we attend to all the input feature vectors, but with different weight!","title":"Attention!"},{"location":"blog/2019/basics-attention/#differences-with-the-encoder-decoder-network","text":"In the Encoder-Decoder network that we discussed in the previous post, the Decoder hidden state is computed as: $$ \\begin{equation} s_{i} = f(s_{i-1}, y_{i-1}) \\end{equation} $$ In the Attention extension, we take the Context vector in computing the Decoder hidden state: $$ \\begin{equation} s_{i} = f(s_{i-1}, y_{i-1}, C_{i}) \\end{equation} $$ The Context vector is the summary of only the most relevant input feature vectors. To capture this relevance , let's consider a variable \\(\\alpha\\) where \\(\\alpha_{i}\\) represents the weight of the encoded representation (also referred to as the annotation ) \\(h_{i}\\) in the Context vector \\(C_{i}\\) - for predicting the output at time \\(i\\) . Given this \\(\\alpha\\) , we can compute the Context vector as: $$ \\begin{equation} C_{i} = \\sum_{j=1}^{T} \\alpha_{i,j} \\cdot h_{j} \\end{equation} $$ \\[ \\begin{equation} \\sum_{j=1}^{T} \\alpha_{i,j} = 1 \\end{equation} \\] To compute \\(\\alpha_{i,j}\\) , we need \\(e_{i,j}\\) - the importance of the \\(j^{th}\\) annotation vector for predicting the \\(i^{th}\\) output symbol. This is what the compatibility function produces. The weight \\(\\alpha_{i,j}\\) of each annotation \\(h_{j}\\) is computed as: $$ \\begin{equation} \\alpha_{i,j} = Softmax(e_{i,j}) = \\frac{e^{e_{i,j}}}{\\sum_{k=1}^{T} e^{e_{i,k}}} \\end{equation} $$ $$ \\begin{equation} \\sum_{j=1}^{T} e_{i,j} \\neq 1 \\end{equation} $$ Where \\(e_{i,j} = a(s_{i-1}, h_j)\\) , \\(a\\) is a compatibility function which computes the importance of each annotation \\(h_j\\) with the Decoder hidden state \\(s_{i-1}\\) . In all our Attention models, it is this function \\(a()\\) that is going to be different. \\(a()\\) defines what type of Attention it is. {: .notice--info} This image summarizes the Attention mechanism. Observe each annotation vector is scaled by the attention weight \\(\\alpha_{i,j}\\) In the Encoder-Decoder network - Given the Decoder hidden representation \\(s_{i-1}\\) (from the previous output time) and the output symbol \\(y_{i-1}\\) (the previous output symbol), we can predict the output symbol at the current time step as: $$ \\begin{equation} p(y_{i} | {y_1, y_2, .........., y_{i-1}}) = g(y_{i-1}, s_i) \\end{equation} $$ Where \\(g()\\) is the entire Decoder function. In the Attention extension - Given the Context vector \\(C_{i}\\) , the Decoder hidden representation \\(s_{i-1}\\) (from the previous output time) and the output symbol \\(y_{i-1}\\) (the previous output symbol), we can predict the output symbol at the current time step as: $$ \\begin{equation} p(y_{i} | {y_1, y_2, .........., y_{i-1}}, C_{i}) = g(y_{i-1}, s_i, C_{i}) \\end{equation} $$ Where \\(g()\\) is the entire Decoder function. The probability of the full output sequence \\(y\\) can be computed as: $$ \\begin{equation} p(y) = \\prod_{i=1}^{U} p(y_i | {y_1, y_2, .........., y_{i-1}}, C_{i}) \\end{equation} $$","title":"Differences with the Encoder-Decoder network"},{"location":"blog/2019/basics-attention/#attention-weights-visualization","text":"So, what does Attention even look like? I trained an Attention model on the TIMIT dataset using the ESPnet toolkit and visualized the weights for 20 epochs and this is what it looks like for the speaker FJSJ0 and utterance SX404 of TIMIT: Word transcript for FJSJ0_SX404 : Barb burned paper and leaves in a big bonfire. {: .notice--info} Phoneme transcript for FJSJ0_SX404 : sil b aa r sil b er n sil p ey sil p er n l iy v z ih n ah sil b ih sil b aa n f ay er sil {: .notice--info}","title":"Attention weights visualization"},{"location":"blog/2019/basics-attention/#phoneme-decoding-final-weights","text":"","title":"Phoneme decoding - final weights"},{"location":"blog/2019/basics-attention/#character-decoding-final-weights","text":"On the \\(x\\) axis from left to right is the Encoder index ranging from \\(0\\) to \\(T\\) , where \\(T\\) is the length of the input feature vector sequence. On the \\(y\\) axis from top to bottom is the Decoder index ranging from \\(0\\) to \\(U\\) , where \\(U\\) is the length of the output symbol sequence. Here, you can see that each row corresponds to the weight for each input feature vector \\(h_{t}\\) in producing the Context vector \\(C_{i}\\) for generating the output symbol \\(y_{i}\\) . If you see the Attention weights before the model is trained (at epoch 0), the Attention weights are all random and hence the Context vector \\(C_{i}\\) contains unnecessary noise from irrelevant input feature vectors. This leads to a degraded performance of the model. It is fairly evident that a good Attention model produces a better Context vector which leads to better model performance.","title":"Character decoding - final weights"},{"location":"blog/2019/basics-attention/#phoneme-decoding-initial-weights","text":"","title":"Phoneme decoding - initial weights"},{"location":"blog/2019/basics-attention/#character-decoding-initial-weights","text":"","title":"Character decoding - initial weights"},{"location":"blog/2019/basics-attention/#attention-weights-for-single-output-symbol","text":"I'm working on visualizing the Attention weights over a Spectrogram every time an output symbol is generated. Ideally it should look like a Gaussian distribution with it's mean at the most relevant \\(h_{t}\\) for generating \\(y_{i}\\) and it's variance proportional to the duration of the phoneme utterance. This is proving more involved than I initially thought, requiring changes to the ESPnet code at a deeper level. I will update this post when I have that. Update: If we plot the Attention weights over the annotation sequence \\(h_{t}\\) for generating each \\(y_{i}\\) , we could see how Attention is playing a role in producing the Context vector \\(C_{i}\\) . Here's what the Attention weights look like for generating each \\(y_{i}\\) at epoch 1. Here's what the Attention weights look like for generating each \\(y_{i}\\) at epoch 20. We could also see how Attention weights progress over time (epochs) to get deeper understanding of how the model is learning. I did just that combining all the Attention weights from each epoch into a gif. Here's what it looks like:","title":"Attention weights for single output symbol"},{"location":"blog/2019/basics-attention/#phoneme-decoding-attention-weights-over-epochs","text":"","title":"Phoneme decoding - Attention weights over epochs"},{"location":"blog/2019/basics-attention/#character-decoding-attention-weights-over-epochs","text":"","title":"Character decoding - Attention weights over epochs"},{"location":"blog/2019/basics-attention/#before-we-start-with-the-different-attention-models","text":"In all the subsequent discussions of Attention models, I would like to follow some consistency. For example, anything that is orange in color is related to the Encoder side of the network, blue with Decoder side and green with the Attention function itself. We will see often that the representation (annotation) learnt by the Encoder, the hidden state of the Decoder and the representations learnt by the Attention function are of different dimensions. This means we can't add them or take dot product. \\[ \\begin{equation} dim(h_{t}) \\ne dim(s_{i}) \\ne dim(f(e)) \\end{equation} \\] To overcome this issue, we project each of these vectors to a fixed dimension and this non-linear projection is learnt along with the other parameters of the network. In the next post we will discuss about the different Attention models available in the ESPnet toolkit.","title":"Before we start with the different Attention models"},{"location":"blog/2019/encoder-decoder-basics/","text":"Encoder-Decoder framework for Speech Recognition In most of the problems we are trying to solve with Machine Learning/Deep Learning, we have a set of inputs \\(x = (x_{1}, x_{2}, .........., x_{T})\\) that we would like to map to a set of outputs \\(y = (y_{1}, y_{2}, .........., y_{T})\\) . Mostly, each input \\(x_{i}\\) corresponds to an output \\(y_{i}\\) . We assume there is some function \\(f()\\) that can map all of these \\(x{i}\\) to their corresponding \\(y_{i}\\) \\[ \\begin{equation} y_{i} = f(x_{i}, \\theta) \\end{equation} \\] When we have this data (x-y pairing), a Supervised learning algorithm can be used to train a model to approximate this mapping function \\(f()\\) . And it has been many people's mission to build algorithms to approximate this function \\(f()\\) as accurately as possible. Problem of Speech The problem with Speech is, our feature vectors are taken from a short time spectra of speech signal. Which means we have a feature vector for every 20-25ms or so. So, what is the problem? The problem is, first : we don't know where the boundary is between one sound (phoneme) and another. If you could open a speech signal in any signal processing software and cut the signal to ~200-300ms and listened to it without any context, it is not possible to distinguish which sound it is. Context matters! {: .notice--info} Second : Intra- and inter-speaker variability - the way one person pronounces a word is different than another person. There are many reasons for this, they may speak different primary language (L1 effect on L2), their vocal tract characteristics are different, gender, age, etc. everything plays a role in how we pronounce words. Third : the problem with the language itself. If we take English for example. phonemes is pronounced as \u02c8f\u014dn\u0113m . Where is the sound f represented in the word phoneme?. (There are other practical issues that are not relevant to this discussion) That being said, let's consider only the first problem for now , where we don't know what sound it is when given an isolated chunk of 100-300ms speech signal. When we collect data to train a model to do Speech recognition, we might give people a previously decided transcript and ask them to read it, then record their speech to get speech data. Or we might collect already existing speech-text pair of data (Audiobooks, Broadcast news recording etc.) to train our model. Observe, in either case, the data we have is the speech signal and the corresponding transcript at the word level . That is, we do not have data about where a word (or a phoneme) ends and where another begins. How do we train a model when we don't even know the \\(x - y\\) pair? This challenging problem of sequence modeling has been the interest of speech community since many decades. There have been many approaches to tackle this problem, two of the recent ones are: Connectionist temporal classification and the Encoder-Decoder approach. Encoder-Decoder network \\(x = (x_{1}, x_{2}, .........., x_{T})\\) be a length \\(T\\) input feature vector sequence to the Encoder network. \\(y = (y_{1}, y_{2}, .........., y_{U})\\) be a length \\(U\\) output symbol sequence the Decoder (also called the Generator) network generates. \\(h = (h_{1}, h_{2}, .........., h_{T})\\) be the Encoder network output which is the encoded hidden vector sequence of length \\(T\\) . Each encoded representation \\(h_{t}\\) contains information about the input sequence with focus on the \\(t^{th}\\) input of the sequence. \\[ \\begin{equation} h_{t} = f(x_{t}, h_{t-1}) \\end{equation} \\] is the hidden state at time \\(t\\) , where \\(f()\\) is some function the Encoder is implementing to update it's hidden representation. In the Encoder-Decoder framework, the Encoder tries to summarize the entire input sequence in a fixed dimension vector \\(h_{t}\\) . The Encoder itself is a Recurrent neural network (RNN/LSTM/BLSTM/GRU) which takes each input feature vector \\(x_{t}\\) and switches it's internal state to represent (summarize) the sequence till that time inside \\(h_{t}\\) . We could take \\(h_{t}\\) at every time step to make a prediction (or not), but we shall wait till the end of the sequence at time \\(T\\) and take the representation \\(h_{T}\\) to start generating our output sequence. This is because we don't know the word/phoneme boundaries and we are hoping the Encoder is able to summarize the input sequence entirely inside \\(h_{T}\\) . We give as input a \\<sos> - start of the sequence token to the Decoder for consistency and to start generating output symbols. The Decoder is another Recurrent neural network (not bidirectional) which switches it's internal state every time to predict the output. At every time step, we feed the output from the previous time step to predict the current output. \\[ \\begin{equation} s_{i} = f(s_{i-1}, y_{i-1}) \\end{equation} \\] is the Decoder hidden state when predicting \\(i^{th}\\) output symbol, where \\(f()\\) is some function the Decoder LSTM is implementing to update it's hidden representation. Given the Decoder hidden representation \\(s_{i-1}\\) (from the previous output time) and the output symbol \\(y_{i-1}\\) (the previous output symbol), we can predict the output symbol at the current time step as: \\[ \\begin{equation} p(y_{i} | \\{y_1, y_2, .........., y_{i-1}\\}) = g(y_{i-1}, s_i) \\end{equation} \\] Where \\(g()\\) is the entire Decoder function. The probability of the full output sequence \\(y\\) can be computed as: \\[ \\begin{equation} p(y) = \\prod_{i=1}^{U} p(y_i | \\{y_1, y_2, .........., y_{i-1}\\}, s_{i}) \\end{equation} \\] We will stop generating the output symbol sequence when the Decoder generates an \\<eos> - end of sequence token. Summary \\(x = (x_{1}, x_{2}, .........., x_{T})\\) is the input sequence Encoder: Summarizes the entire input sequence \\(x\\) inside \\(h_{T}\\) \\(h = (h_{1}, h_{2}, .........., h_{T})\\) is the hidden vector sequence \\(h_{t}\\) : Summary of the input sequence till time \\(t\\) Decoder: Generates the output sequence given \\(h_{T}\\) \\(y = (y_{1}, y_{2}, .........., y_{U})\\) is the output sequence In the next post we will discuss about the basics of the Attention extension to the Encoder-Decoder framework and how it is better.","title":"Encoder-Decoder framework for Speech Recognition"},{"location":"blog/2019/encoder-decoder-basics/#encoder-decoder-framework-for-speech-recognition","text":"In most of the problems we are trying to solve with Machine Learning/Deep Learning, we have a set of inputs \\(x = (x_{1}, x_{2}, .........., x_{T})\\) that we would like to map to a set of outputs \\(y = (y_{1}, y_{2}, .........., y_{T})\\) . Mostly, each input \\(x_{i}\\) corresponds to an output \\(y_{i}\\) . We assume there is some function \\(f()\\) that can map all of these \\(x{i}\\) to their corresponding \\(y_{i}\\) \\[ \\begin{equation} y_{i} = f(x_{i}, \\theta) \\end{equation} \\] When we have this data (x-y pairing), a Supervised learning algorithm can be used to train a model to approximate this mapping function \\(f()\\) . And it has been many people's mission to build algorithms to approximate this function \\(f()\\) as accurately as possible.","title":"Encoder-Decoder framework for Speech Recognition"},{"location":"blog/2019/encoder-decoder-basics/#problem-of-speech","text":"The problem with Speech is, our feature vectors are taken from a short time spectra of speech signal. Which means we have a feature vector for every 20-25ms or so. So, what is the problem? The problem is, first : we don't know where the boundary is between one sound (phoneme) and another. If you could open a speech signal in any signal processing software and cut the signal to ~200-300ms and listened to it without any context, it is not possible to distinguish which sound it is. Context matters! {: .notice--info} Second : Intra- and inter-speaker variability - the way one person pronounces a word is different than another person. There are many reasons for this, they may speak different primary language (L1 effect on L2), their vocal tract characteristics are different, gender, age, etc. everything plays a role in how we pronounce words. Third : the problem with the language itself. If we take English for example. phonemes is pronounced as \u02c8f\u014dn\u0113m . Where is the sound f represented in the word phoneme?. (There are other practical issues that are not relevant to this discussion) That being said, let's consider only the first problem for now , where we don't know what sound it is when given an isolated chunk of 100-300ms speech signal. When we collect data to train a model to do Speech recognition, we might give people a previously decided transcript and ask them to read it, then record their speech to get speech data. Or we might collect already existing speech-text pair of data (Audiobooks, Broadcast news recording etc.) to train our model. Observe, in either case, the data we have is the speech signal and the corresponding transcript at the word level . That is, we do not have data about where a word (or a phoneme) ends and where another begins. How do we train a model when we don't even know the \\(x - y\\) pair? This challenging problem of sequence modeling has been the interest of speech community since many decades. There have been many approaches to tackle this problem, two of the recent ones are: Connectionist temporal classification and the Encoder-Decoder approach.","title":"Problem of Speech"},{"location":"blog/2019/encoder-decoder-basics/#encoder-decoder-network","text":"\\(x = (x_{1}, x_{2}, .........., x_{T})\\) be a length \\(T\\) input feature vector sequence to the Encoder network. \\(y = (y_{1}, y_{2}, .........., y_{U})\\) be a length \\(U\\) output symbol sequence the Decoder (also called the Generator) network generates. \\(h = (h_{1}, h_{2}, .........., h_{T})\\) be the Encoder network output which is the encoded hidden vector sequence of length \\(T\\) . Each encoded representation \\(h_{t}\\) contains information about the input sequence with focus on the \\(t^{th}\\) input of the sequence. \\[ \\begin{equation} h_{t} = f(x_{t}, h_{t-1}) \\end{equation} \\] is the hidden state at time \\(t\\) , where \\(f()\\) is some function the Encoder is implementing to update it's hidden representation. In the Encoder-Decoder framework, the Encoder tries to summarize the entire input sequence in a fixed dimension vector \\(h_{t}\\) . The Encoder itself is a Recurrent neural network (RNN/LSTM/BLSTM/GRU) which takes each input feature vector \\(x_{t}\\) and switches it's internal state to represent (summarize) the sequence till that time inside \\(h_{t}\\) . We could take \\(h_{t}\\) at every time step to make a prediction (or not), but we shall wait till the end of the sequence at time \\(T\\) and take the representation \\(h_{T}\\) to start generating our output sequence. This is because we don't know the word/phoneme boundaries and we are hoping the Encoder is able to summarize the input sequence entirely inside \\(h_{T}\\) . We give as input a \\<sos> - start of the sequence token to the Decoder for consistency and to start generating output symbols. The Decoder is another Recurrent neural network (not bidirectional) which switches it's internal state every time to predict the output. At every time step, we feed the output from the previous time step to predict the current output. \\[ \\begin{equation} s_{i} = f(s_{i-1}, y_{i-1}) \\end{equation} \\] is the Decoder hidden state when predicting \\(i^{th}\\) output symbol, where \\(f()\\) is some function the Decoder LSTM is implementing to update it's hidden representation. Given the Decoder hidden representation \\(s_{i-1}\\) (from the previous output time) and the output symbol \\(y_{i-1}\\) (the previous output symbol), we can predict the output symbol at the current time step as: \\[ \\begin{equation} p(y_{i} | \\{y_1, y_2, .........., y_{i-1}\\}) = g(y_{i-1}, s_i) \\end{equation} \\] Where \\(g()\\) is the entire Decoder function. The probability of the full output sequence \\(y\\) can be computed as: \\[ \\begin{equation} p(y) = \\prod_{i=1}^{U} p(y_i | \\{y_1, y_2, .........., y_{i-1}\\}, s_{i}) \\end{equation} \\] We will stop generating the output symbol sequence when the Decoder generates an \\<eos> - end of sequence token.","title":"Encoder-Decoder network"},{"location":"blog/2019/encoder-decoder-basics/#summary","text":"\\(x = (x_{1}, x_{2}, .........., x_{T})\\) is the input sequence Encoder: Summarizes the entire input sequence \\(x\\) inside \\(h_{T}\\) \\(h = (h_{1}, h_{2}, .........., h_{T})\\) is the hidden vector sequence \\(h_{t}\\) : Summary of the input sequence till time \\(t\\) Decoder: Generates the output sequence given \\(h_{T}\\) \\(y = (y_{1}, y_{2}, .........., y_{U})\\) is the output sequence In the next post we will discuss about the basics of the Attention extension to the Encoder-Decoder framework and how it is better.","title":"Summary"},{"location":"blog/2019/tensorflow-dataset/","text":"When I saw the TensorFlow Dev Summit 2019, the thing that I wanted to try out the most was the new tf.data.Dataset API . We all know how painful it is to feed data to our models in an efficient way. This is especially true if you're working with Speech. For my work with Pointer-Networks , I was using PyTorch's DataLoader to feed data to my models. This always left something to be desired (a discussion for another day). I was on the lookout for a different (hopefully better) way to feed data to my models when I heard about tf.data.Dataset. This post is my exploration of the API, I will try to keep this post updated as I go about my exploration. I am exploring the following APIs: tf.data.Dataset.from_generator , tf.data.Options , tf.data.TFRecordDataset and all the other experimental features! tf.data.Dataset.from_generator tf.data.Dataset.from_generator is the function to use if your data pipeline does not fit into any of the other methods. Since I mostly work with speech, I need a way to load my data from disk batch-by-batch. I can't fit all my data into memory because it's just too big (typically couple of 100 GiBs ). One way to feed such dataset to my models is by loading the data batch-by-batch from the disk instead of loading everything at once and iterating over it. This has always been one of the most difficult part of my model building experience in Speech. Having an efficient data pipeline makes my life easier :). ESPnet did just that by using the ark file splits generated by kaldi to load the batches and feed them to my models. This is definitely not THE solution to the problem, but it got the job done. I believe the tf.data.Dataset.from_generator is the way to go for my data pipeline. Now, let's say I need to solve the problem of finding ConvexHull points from a sequence of points. This is one of the problems the original Pointer-Networks paper tried to solve. Instead of using the dataset that the authors provided, I want to generate my own dataset (because why not? how difficult could it be to generate a set of points to solve this problem?). By generating my own dataset, I can practically have infinite training examples and full control over what I want to do with it. For this reason alone, I can't use the other methods as I will have to store the training examples in memory. I need to generate my examples on-the-go. tf.data.Dataset.from_generator solves this exact problem. How to use it? Before we even start feeding data to our model, we need to have a python generator function which generates one training pair needed for our model. What this means is, there should be a function which has a yield statement instead of a return statement. This does not mean there can't be a return statement, in a generator function there could be multiple yields and returns. Let's say our dataset is of 1000 images of size 28x28 and belong to one of 10 classes. Our generator function might look something like this except we will be reading the images from disk. Python 1 2 3 4 5 def our_generator (): for i in range ( 1000 ): x = np . random . rand ( 28 , 28 ) y = np . random . randint ( 1 , 10 , size = 1 ) yield x , y We could build our TensorFlow dataset with this generator function. The tf.data.Dataset.from_generator function has the following arguments: Python 1 2 3 4 5 6 def from_generator ( generator , output_types , output_shapes = None , args = None ) While the output_shapes is optional, we need to specify the output_types. In this particular case the first returned value is a 2D array of floats and the second value is a 1D array of integers. Our dataset object will look something like this: Python 1 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) To use this dataset in our model training, we need to either use the make_one_shot_iterator (which is being deprecated) or use the dataset in our training loop. Using make_one_shot_iterator Python 1 2 3 4 5 6 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) iterator = dataset . make_one_shot_iterator () x , y = iterator . get_next () print ( x . shape , y . shape ) #(28, 28) (1,) Loop over the dataset object in our training loop Python 1 2 3 4 5 6 7 for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , epoch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 999 #Data shape: (28, 28) (1,) tf.data.Dataset options - batch, repeat, shuffle tf.data.Dataset comes with a couple of options to make our lives easier. If you see our previous example, we get one example every time we call the dataset object. What if we would want a batch of examples, or if we want to iterate over the dataset many times, or if we want to shuffle the dataset after every epoch. Using the batch, repeat, and shuffle function we could achieve this. Python 1 2 3 4 5 6 7 def our_generator (): for i in range ( 1000 ): x = np . random . rand ( 28 , 28 ) y = np . random . randint ( 1 , 10 , size = 1 ) yield x , y dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) batch Python 1 2 3 4 5 6 7 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) dataset = dataset . batch ( 10 ) iterator = dataset . make_one_shot_iterator () x , y = iterator . get_next () print ( x . shape , y . shape ) #(10, 28, 28) (10, 1) Now, every time we use the dataset object, the generator function is called 10 times. The batch function combines consecutive elements of this dataset into batches. If we reach the end of the dataset and the batch is less than the batch_size specified, we can pass the argument drop_remainder=True to ignore that particular batch. Python 1 2 3 4 5 6 7 for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , epoch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 99 #Data shape: (10, 28, 28) (10, 1) repeat Python 1 2 3 4 5 6 7 8 9 10 11 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) dataset = dataset . batch ( batch_size = 10 ) dataset = dataset . repeat ( count = 2 ) for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , batch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 199 #Data shape: (10, 28, 28) (10, 1) Here, the dataset is looped over 2 times. Hence we get twice the number of batches for training. If we want to repeat the dataset indefinitely, we should set the argument to count=-1 shuffle Python 1 2 3 4 5 6 7 8 9 10 11 12 13 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) dataset = dataset . batch ( batch_size = 10 ) dataset = dataset . repeat ( count = 2 ) dataset = dataset . shuffle ( buffer_size = 1000 ) iterator = dataset . make_one_shot_iterator () for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , batch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 199 #Data shape: (10, 28, 28) (10, 1) Here, the argument buffer_size=100 specifies the number of elements from this dataset from which the new dataset will sample. Essentially, this fills the dataset with buffer_size elements, then randomly samples elements from this buffer. Use buffer_size>=dataset_size for perfect shuffling. Other options In addition to batch, repeat, and shuffle, there are many other functions the TensorFlow Dataset API comes with. I will update this post with options like - map , reduce , with_options Conclusion tf.data.Dataset potentially can solve most of my data pipeline woes. I will test how I can use this to feed speech data (use a py_function to do feature extraction) to my models, and using the map function to augment the dataset (adding noise, combining files, time scaling etc). You can use this notebook to play around with the functions that I have used. https://colab.research.google.com/drive/1XxHNtgwFVZzILlOwEhvsYuov5s1MAy2N","title":"Tensorflow 2.0 tf.data.Dataset.from_generator"},{"location":"blog/2019/tensorflow-dataset/#tfdatadatasetfrom_generator","text":"tf.data.Dataset.from_generator is the function to use if your data pipeline does not fit into any of the other methods. Since I mostly work with speech, I need a way to load my data from disk batch-by-batch. I can't fit all my data into memory because it's just too big (typically couple of 100 GiBs ). One way to feed such dataset to my models is by loading the data batch-by-batch from the disk instead of loading everything at once and iterating over it. This has always been one of the most difficult part of my model building experience in Speech. Having an efficient data pipeline makes my life easier :). ESPnet did just that by using the ark file splits generated by kaldi to load the batches and feed them to my models. This is definitely not THE solution to the problem, but it got the job done. I believe the tf.data.Dataset.from_generator is the way to go for my data pipeline. Now, let's say I need to solve the problem of finding ConvexHull points from a sequence of points. This is one of the problems the original Pointer-Networks paper tried to solve. Instead of using the dataset that the authors provided, I want to generate my own dataset (because why not? how difficult could it be to generate a set of points to solve this problem?). By generating my own dataset, I can practically have infinite training examples and full control over what I want to do with it. For this reason alone, I can't use the other methods as I will have to store the training examples in memory. I need to generate my examples on-the-go. tf.data.Dataset.from_generator solves this exact problem.","title":"tf.data.Dataset.from_generator"},{"location":"blog/2019/tensorflow-dataset/#how-to-use-it","text":"Before we even start feeding data to our model, we need to have a python generator function which generates one training pair needed for our model. What this means is, there should be a function which has a yield statement instead of a return statement. This does not mean there can't be a return statement, in a generator function there could be multiple yields and returns. Let's say our dataset is of 1000 images of size 28x28 and belong to one of 10 classes. Our generator function might look something like this except we will be reading the images from disk. Python 1 2 3 4 5 def our_generator (): for i in range ( 1000 ): x = np . random . rand ( 28 , 28 ) y = np . random . randint ( 1 , 10 , size = 1 ) yield x , y We could build our TensorFlow dataset with this generator function. The tf.data.Dataset.from_generator function has the following arguments: Python 1 2 3 4 5 6 def from_generator ( generator , output_types , output_shapes = None , args = None ) While the output_shapes is optional, we need to specify the output_types. In this particular case the first returned value is a 2D array of floats and the second value is a 1D array of integers. Our dataset object will look something like this: Python 1 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) To use this dataset in our model training, we need to either use the make_one_shot_iterator (which is being deprecated) or use the dataset in our training loop.","title":"How to use it?"},{"location":"blog/2019/tensorflow-dataset/#using-make_one_shot_iterator","text":"Python 1 2 3 4 5 6 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) iterator = dataset . make_one_shot_iterator () x , y = iterator . get_next () print ( x . shape , y . shape ) #(28, 28) (1,)","title":"Using make_one_shot_iterator"},{"location":"blog/2019/tensorflow-dataset/#loop-over-the-dataset-object-in-our-training-loop","text":"Python 1 2 3 4 5 6 7 for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , epoch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 999 #Data shape: (28, 28) (1,)","title":"Loop over the dataset object in our training loop"},{"location":"blog/2019/tensorflow-dataset/#tfdatadataset-options-batch-repeat-shuffle","text":"tf.data.Dataset comes with a couple of options to make our lives easier. If you see our previous example, we get one example every time we call the dataset object. What if we would want a batch of examples, or if we want to iterate over the dataset many times, or if we want to shuffle the dataset after every epoch. Using the batch, repeat, and shuffle function we could achieve this. Python 1 2 3 4 5 6 7 def our_generator (): for i in range ( 1000 ): x = np . random . rand ( 28 , 28 ) y = np . random . randint ( 1 , 10 , size = 1 ) yield x , y dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 ))","title":"tf.data.Dataset options - batch, repeat, shuffle"},{"location":"blog/2019/tensorflow-dataset/#batch","text":"Python 1 2 3 4 5 6 7 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) dataset = dataset . batch ( 10 ) iterator = dataset . make_one_shot_iterator () x , y = iterator . get_next () print ( x . shape , y . shape ) #(10, 28, 28) (10, 1) Now, every time we use the dataset object, the generator function is called 10 times. The batch function combines consecutive elements of this dataset into batches. If we reach the end of the dataset and the batch is less than the batch_size specified, we can pass the argument drop_remainder=True to ignore that particular batch. Python 1 2 3 4 5 6 7 for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , epoch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 99 #Data shape: (10, 28, 28) (10, 1)","title":"batch"},{"location":"blog/2019/tensorflow-dataset/#repeat","text":"Python 1 2 3 4 5 6 7 8 9 10 11 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) dataset = dataset . batch ( batch_size = 10 ) dataset = dataset . repeat ( count = 2 ) for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , batch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 199 #Data shape: (10, 28, 28) (10, 1) Here, the dataset is looped over 2 times. Hence we get twice the number of batches for training. If we want to repeat the dataset indefinitely, we should set the argument to count=-1","title":"repeat"},{"location":"blog/2019/tensorflow-dataset/#shuffle","text":"Python 1 2 3 4 5 6 7 8 9 10 11 12 13 dataset = tf . data . Dataset . from_generator ( our_generator , ( tf . float32 , tf . int16 )) dataset = dataset . batch ( batch_size = 10 ) dataset = dataset . repeat ( count = 2 ) dataset = dataset . shuffle ( buffer_size = 1000 ) iterator = dataset . make_one_shot_iterator () for batch , ( x , y ) in enumerate ( dataset ): pass print ( \"batch: \" , batch ) print ( \"Data shape: \" , x . shape , y . shape ) #batch: 199 #Data shape: (10, 28, 28) (10, 1) Here, the argument buffer_size=100 specifies the number of elements from this dataset from which the new dataset will sample. Essentially, this fills the dataset with buffer_size elements, then randomly samples elements from this buffer. Use buffer_size>=dataset_size for perfect shuffling.","title":"shuffle"},{"location":"blog/2019/tensorflow-dataset/#other-options","text":"In addition to batch, repeat, and shuffle, there are many other functions the TensorFlow Dataset API comes with. I will update this post with options like - map , reduce , with_options","title":"Other options"},{"location":"blog/2019/tensorflow-dataset/#conclusion","text":"tf.data.Dataset potentially can solve most of my data pipeline woes. I will test how I can use this to feed speech data (use a py_function to do feature extraction) to my models, and using the map function to augment the dataset (adding noise, combining files, time scaling etc). You can use this notebook to play around with the functions that I have used. https://colab.research.google.com/drive/1XxHNtgwFVZzILlOwEhvsYuov5s1MAy2N","title":"Conclusion"}]}